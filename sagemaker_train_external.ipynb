{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket  = 'YOUR_BUCKET'\n",
    "prefix  = 'sagemaker/3d-densenet'\n",
    "dataset = 'kth'\n",
    "\n",
    "staging_dir = 'stage'\n",
    "export_dir  = 'export'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import errno\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "print('Using Tensorflow {0}'.format(tf.__version__))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "num_frames_per_clip = 16 # The number of frames for a video clip\n",
    "skip_frames = 10 # The number of frames to skip when we process the video\n",
    "train_eval_split_factor =  0.75 # Use this factor to split the train (3/4) and eval data (1/4)\n",
    "width = 128 # Image width\n",
    "height = 128 # Image height\n",
    "quality = 100 # Image quality\n",
    "channel = 3 # Image color channel\n",
    "\n",
    "def get_clips(image_list):\n",
    "    # Given a list of images, return video clips of (num_frames_per_clip) consecutive frames as a list.\n",
    "    video_clips = []\n",
    "    images_len = len(image_list)\n",
    "    if images_len < num_frames_per_clip:\n",
    "        return video_clips\n",
    "\n",
    "    # Prepare the first clip\n",
    "    video_clips.append(image_list[:num_frames_per_clip])\n",
    "\n",
    "    num_of_extra_clip = int((images_len - num_frames_per_clip) / skip_frames)\n",
    "    for i in range(1, num_of_extra_clip + 1):\n",
    "        start = i * skip_frames - 1\n",
    "        end = start + num_frames_per_clip\n",
    "        video_clips.append(image_list[start:end])\n",
    "\n",
    "    return video_clips\n",
    "\n",
    "def download_videos(origin_videos_location):    \n",
    "    resp = s3.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=origin_videos_location)\n",
    "    \n",
    "    video_filenames = []\n",
    "    for obj in resp['Contents']:\n",
    "        # Create target directory & all intermediate directories if don't exists\n",
    "        path, filename = os.path.split(obj['Key'])\n",
    "        if not filename.endswith(('.avi', '.mp4')): continue\n",
    "        try:\n",
    "            os.makedirs('{0}/{1}'.format(staging_dir, path))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "        s3.download_file(bucket, obj['Key'], '{0}/{1}'.format(staging_dir, obj['Key']))\n",
    "        \n",
    "def process_dataset(train_writer, eval_writer, origin_videos_location):\n",
    "    download_videos(origin_videos_location)\n",
    "    data_dir = os.path.join(os.path.join(os.getcwd(), staging_dir), origin_videos_location)\n",
    "    label = -1\n",
    "    # [class1, class2, class3, ..., class n]\n",
    "    for class_dir in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            # Set the label value for this class, start from 0\n",
    "            label += 1\n",
    "            print(\"Processing class: \" + str(label) + \", name: \" + os.path.basename(class_path))\n",
    "            # Process each video file in this class\n",
    "            video_filenames = os.listdir(class_path)\n",
    "            \n",
    "            for video_filename in video_filenames[0:int(\n",
    "                    train_eval_split_factor * len(video_filenames))]:\n",
    "                process_video(train_writer, class_path, video_filename, label)\n",
    "            for video_filename in video_filenames[\n",
    "                    int(train_eval_split_factor *\n",
    "                        len(video_filenames)):len(video_filenames)]:\n",
    "                process_video(eval_writer, class_path, video_filename, label)        \n",
    "\n",
    "def process_video(writer, class_path, video_filename, label):\n",
    "    video_filename_path = os.path.join(class_path, video_filename)\n",
    "    if video_filename_path.endswith(('.avi', '.mp4')):\n",
    "        video_clips = _convert_video_to_clips(video_filename_path)\n",
    "        # Convert the clip to tf record\n",
    "        for clip in video_clips:\n",
    "            tf_example = create_tf_example(raw=clip, label=label)\n",
    "            writer.write(tf_example.SerializeToString())   \n",
    "            \n",
    "def _convert_video_to_clips(video_path):\n",
    "    # Use opencv to read video to list of images\n",
    "    video_images_list = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        # frame shape [height, width, channel]\n",
    "        _, frame = cap.read()\n",
    "        try:\n",
    "            # pil_image shape [width, height, channel]\n",
    "            pil_image = Image.fromarray(frame)\n",
    "            # Resize the image and convert the image according to the channel information\n",
    "            if channel == 1:\n",
    "                pil_image = pil_image.resize((width, height),\n",
    "                                             Image.NEAREST).convert('L')\n",
    "            else:\n",
    "                pil_image = pil_image.resize((width, height),\n",
    "                                             Image.NEAREST)\n",
    "            # Encode the image to JPEG\n",
    "            with BytesIO() as buffer:\n",
    "                pil_image.save(buffer, format=\"JPEG\", quality=quality)\n",
    "                video_images_list.append(buffer.getvalue())\n",
    "        except AttributeError:\n",
    "            # Fail to read the image\n",
    "            break\n",
    "\n",
    "    # Convert list of images to clips of images with type np.float32\n",
    "    return get_clips(image_list=video_images_list)\n",
    "\n",
    "def _bytelist_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def _floatlist_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_tf_example(raw, label):\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'clip/width': _int64_feature(width),\n",
    "                'clip/height': _int64_feature(height),\n",
    "                'clip/channel': _int64_feature(channel),\n",
    "                'clip/raw': _bytelist_feature(raw),\n",
    "                'clip/label': _int64_feature(label)\n",
    "            }))\n",
    "\n",
    "def get_total_video_clip_number(data_path):\n",
    "    count = 0\n",
    "    for _ in tf.python_io.tf_record_iterator(data_path):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '{0}/{1}'.format(staging_dir, 'tfrecords')\n",
    "train_data_path = '{0}/{1}'.format(data_path, 'train.tfrecord') # Path to output train TFRecord\n",
    "eval_data_path  = '{0}/{1}'.format(data_path, 'eval.tfrecord')  # Path to output eval TFRecord\n",
    "\n",
    "os.makedirs(data_path)\n",
    "\n",
    "# Write the dataset\n",
    "train_writer = tf.python_io.TFRecordWriter(train_data_path)\n",
    "eval_writer = tf.python_io.TFRecordWriter(eval_data_path)\n",
    "\n",
    "process_dataset(\n",
    "    train_writer=train_writer,\n",
    "    eval_writer=eval_writer,\n",
    "    origin_videos_location='{0}/{1}'.format(prefix, dataset))\n",
    "\n",
    "train_writer.close()\n",
    "eval_writer.close()\n",
    "\n",
    "# Count the dataset record\n",
    "train_total_video_clip = get_total_video_clip_number(train_data_path)\n",
    "print(\"Total clips in train dataset: \" + str(train_total_video_clip))\n",
    "\n",
    "eval_total_video_clip = get_total_video_clip_number(eval_data_path)\n",
    "print(\"Total clips in eval dataset: \" + str(eval_total_video_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = sagemaker_session.upload_data(\n",
    "    path=train_data_path, bucket=bucket, key_prefix='{0}/train_data'.format(prefix))\n",
    "eval_input = sagemaker_session.upload_data(\n",
    "    path=eval_data_path, bucket=bucket, key_prefix='{0}/train_data'.format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize script variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = 'ml.m5.large'\n",
    "\n",
    "custom_code_upload_location = 's3://{0}/{1}/train_code'.format(bucket, prefix)\n",
    "model_artifacts_location = 's3://{0}/{1}/train_output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit script for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hparams = {\n",
    "    'num_classes': 3,  # The number of the classes that this dataset had\n",
    "    'batch_size': 10,\n",
    "    'initial_learning_rate': 0.1,\n",
    "    'decay_step': 1000,\n",
    "    'lr_decay_factor': 0.1,  # Learning rate will decay by a factor for every decay_step\n",
    "    'growth_rate': 12,  # Grows rate for every layer [12, 24, 40]\n",
    "    'network_depth': 20,  # Depth of the whole network [20, 40, 250]\n",
    "    'total_blocks': 3,  # Total blocks of layers stack\n",
    "    'keep_prob': 0.9,  # Keep probability for dropout\n",
    "    'weight_decay': 1e-4,  # Weight decay for L2 loss\n",
    "    'model_type': 'DenseNet3D',\n",
    "    'reduction': 0.5,  # Reduction rate at transition layer for the models\n",
    "    'bc_mode': True,\n",
    "    'num_frames_per_clip': num_frames_per_clip,  # The length of the video clip\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'channel': channel,\n",
    "    'train_total_video_clip': train_total_video_clip, # This number is for KTH dataset with default setting\n",
    "    'eval_total_video_clip': eval_total_video_clip # This number is for KTH dataset with default setting\n",
    "}\n",
    "\n",
    "action_estimator = TensorFlow(\n",
    "    source_dir='estimator_source',\n",
    "    entry_point='densenet_3d_estimator.py',\n",
    "    role=role,\n",
    "    output_path=model_artifacts_location,\n",
    "    code_location=custom_code_upload_location,\n",
    "    train_instance_count=1,\n",
    "    train_volume_size=30,\n",
    "    framework_version='1.11.0',\n",
    "    train_instance_type=train_instance_type,\n",
    "    training_steps=1,\n",
    "    evaluation_steps=1,\n",
    "    hyperparameters=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_data_location = 's3://{0}/{1}/train_data'.format(bucket, prefix)\n",
    "action_estimator.fit(train_data_location, run_tensorboard_locally=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_predictor = action_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw_video_clips = _convert_video_to_clips('test_videos/person02_running_d1_uncomp.avi')\n",
    "\n",
    "for raw_video_clip in raw_video_clips:\n",
    "    video_clip = []\n",
    "    for raw_image in raw_video_clip:\n",
    "        image = cv2.imdecode(np.frombuffer(raw_image, dtype=np.uint8), 1)\n",
    "        video_clip.append(image)\n",
    "    output_dict = action_predictor.predict( {'video_clips': [video_clip] } )\n",
    "    print(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(os.path.join(os.getcwd(), export_dir)):\n",
    "    shutil.rmtree(export_dir)\n",
    "    \n",
    "if os.path.exists(os.path.join(os.getcwd(), staging_dir)):\n",
    "    shutil.rmtree(staging_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
