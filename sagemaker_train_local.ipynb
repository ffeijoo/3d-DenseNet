{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket  = 'YOUR_BUCKET'\n",
    "prefix  = 'sagemaker/3d-densenet'\n",
    "dataset = 'kth'\n",
    "\n",
    "staging_dir = 'stage'\n",
    "export_dir  = 'export'\n",
    "\n",
    "tensorflow_ver = !pip show tensorflow | grep Version\n",
    "opencv_ver = !pip show opencv-python | grep Version\n",
    "\n",
    "if '1.11.0' not in tensorflow_ver[-1]:\n",
    "    !pip install --upgrade keras==2.2.4\n",
    "    !pip install --upgrade tensorflow==1.11.0\n",
    "\n",
    "if '3.4.3' not in opencv_ver[-1]:\n",
    "    !pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import errno\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "print('Using Tensorflow {0}'.format(tf.__version__))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "num_frames_per_clip = 16 # The number of frames for a video clip\n",
    "skip_frames = 10 # The number of frames to skip when we process the video\n",
    "train_eval_split_factor =  0.75 # Use this factor to split the train (3/4) and eval data (1/4)\n",
    "width = 128 # Image width\n",
    "height = 128 # Image height\n",
    "quality = 100 # Image quality\n",
    "channel = 3 # Image color channel\n",
    "\n",
    "def get_clips(image_list):\n",
    "    # Given a list of images, return video clips of (num_frames_per_clip) consecutive frames as a list.\n",
    "    video_clips = []\n",
    "    images_len = len(image_list)\n",
    "    if images_len < num_frames_per_clip:\n",
    "        return video_clips\n",
    "\n",
    "    # Prepare the first clip\n",
    "    video_clips.append(image_list[:num_frames_per_clip])\n",
    "\n",
    "    num_of_extra_clip = int((images_len - num_frames_per_clip) / skip_frames)\n",
    "    for i in range(1, num_of_extra_clip + 1):\n",
    "        start = i * skip_frames - 1\n",
    "        end = start + num_frames_per_clip\n",
    "        video_clips.append(image_list[start:end])\n",
    "\n",
    "    return video_clips\n",
    "\n",
    "def download_videos(origin_videos_location):    \n",
    "    resp = s3.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=origin_videos_location)\n",
    "    \n",
    "    video_filenames = []\n",
    "    for obj in resp['Contents']:\n",
    "        # Create target directory & all intermediate directories if don't exists\n",
    "        path, filename = os.path.split(obj['Key'])\n",
    "        if not filename.endswith(('.avi', '.mp4')): continue\n",
    "        try:\n",
    "            os.makedirs('{0}/{1}'.format(staging_dir, path))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "        s3.download_file(bucket, obj['Key'], '{0}/{1}'.format(staging_dir, obj['Key']))\n",
    "        \n",
    "def process_dataset(train_writer, eval_writer, origin_videos_location):\n",
    "    download_videos(origin_videos_location)\n",
    "    data_dir = os.path.join(os.path.join(os.getcwd(), staging_dir), origin_videos_location)\n",
    "    label = -1\n",
    "    # [class1, class2, class3, ..., class n]\n",
    "    for class_dir in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            # Set the label value for this class, start from 0\n",
    "            label += 1\n",
    "            print(\"Processing class: \" + str(label) + \", name: \" + os.path.basename(class_path))\n",
    "            # Process each video file in this class\n",
    "            video_filenames = os.listdir(class_path)\n",
    "            \n",
    "            for video_filename in video_filenames[0:int(\n",
    "                    train_eval_split_factor * len(video_filenames))]:\n",
    "                process_video(train_writer, class_path, video_filename, label)\n",
    "            for video_filename in video_filenames[\n",
    "                    int(train_eval_split_factor *\n",
    "                        len(video_filenames)):len(video_filenames)]:\n",
    "                process_video(eval_writer, class_path, video_filename, label)        \n",
    "\n",
    "def process_video(writer, class_path, video_filename, label):\n",
    "    video_filename_path = os.path.join(class_path, video_filename)\n",
    "    if video_filename_path.endswith(('.avi', '.mp4')):\n",
    "        video_clips = _convert_video_to_clips(video_filename_path)\n",
    "        # Convert the clip to tf record\n",
    "        for clip in video_clips:\n",
    "            tf_example = create_tf_example(raw=clip, label=label)\n",
    "            writer.write(tf_example.SerializeToString())   \n",
    "            \n",
    "def _convert_video_to_clips(video_path):\n",
    "    # Use opencv to read video to list of images\n",
    "    video_images_list = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        # frame shape [height, width, channel]\n",
    "        _, frame = cap.read()\n",
    "        try:\n",
    "            # pil_image shape [width, height, channel]\n",
    "            pil_image = Image.fromarray(frame)\n",
    "            # Resize the image and convert the image according to the channel information\n",
    "            if channel == 1:\n",
    "                pil_image = pil_image.resize((width, height),\n",
    "                                             Image.NEAREST).convert('L')\n",
    "            else:\n",
    "                pil_image = pil_image.resize((width, height),\n",
    "                                             Image.NEAREST)\n",
    "            # Encode the image to JPEG\n",
    "            with BytesIO() as buffer:\n",
    "                pil_image.save(buffer, format=\"JPEG\", quality=quality)\n",
    "                video_images_list.append(buffer.getvalue())\n",
    "        except AttributeError:\n",
    "            # Fail to read the image\n",
    "            break\n",
    "\n",
    "    # Convert list of images to clips of images with type np.float32\n",
    "    return get_clips(image_list=video_images_list)\n",
    "\n",
    "def _bytelist_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def _floatlist_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def create_tf_example(raw, label):\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'clip/width': _int64_feature(width),\n",
    "                'clip/height': _int64_feature(height),\n",
    "                'clip/channel': _int64_feature(channel),\n",
    "                'clip/raw': _bytelist_feature(raw),\n",
    "                'clip/label': _int64_feature(label)\n",
    "            }))\n",
    "\n",
    "def get_total_video_clip_number(data_path):\n",
    "    count = 0\n",
    "    for _ in tf.python_io.tf_record_iterator(data_path):\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '{0}/{1}'.format(staging_dir, 'tfrecords')\n",
    "train_data_path = '{0}/{1}'.format(data_path, 'train.tfrecord') # Path to output train TFRecord\n",
    "eval_data_path  = '{0}/{1}'.format(data_path, 'eval.tfrecord')  # Path to output eval TFRecord\n",
    "\n",
    "os.makedirs(data_path)\n",
    "\n",
    "# Write the dataset\n",
    "train_writer = tf.python_io.TFRecordWriter(train_data_path)\n",
    "eval_writer = tf.python_io.TFRecordWriter(eval_data_path)\n",
    "\n",
    "process_dataset(\n",
    "    train_writer=train_writer,\n",
    "    eval_writer=eval_writer,\n",
    "    origin_videos_location='{0}/{1}'.format(prefix, dataset))\n",
    "\n",
    "train_writer.close()\n",
    "eval_writer.close()\n",
    "\n",
    "# Count the dataset record\n",
    "train_total_video_clip = get_total_video_clip_number(train_data_path)\n",
    "print(\"Total clips in train dataset: \" + str(train_total_video_clip))\n",
    "\n",
    "eval_total_video_clip = get_total_video_clip_number(eval_data_path)\n",
    "print(\"Total clips in eval dataset: \" + str(eval_total_video_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import source_dir.densenet_3d_estimator as estimator\n",
    "\n",
    "model_artifacts_location = '{0}/{1}'.format(staging_dir, 'train_output')\n",
    "\n",
    "hparams = {\n",
    "    'num_classes': 3,  # The number of the classes that this dataset had\n",
    "    'batch_size': 10,\n",
    "    'initial_learning_rate': 0.1,\n",
    "    'decay_step': 1000,\n",
    "    'lr_decay_factor': 0.1,  # Learning rate will decay by a factor for every decay_step\n",
    "    'growth_rate': 12,  # Grows rate for every layer [12, 24, 40]\n",
    "    'network_depth': 20,  # Depth of the whole network [20, 40, 250]\n",
    "    'total_blocks': 3,  # Total blocks of layers stack\n",
    "    'keep_prob': 0.9,  # Keep probability for dropout\n",
    "    'weight_decay': 1e-4,  # Weight decay for L2 loss\n",
    "    'model_type': 'DenseNet3D',\n",
    "    'reduction': 0.5,  # Reduction rate at transition layer for the models\n",
    "    'bc_mode': True,\n",
    "    'num_frames_per_clip': num_frames_per_clip,  # The length of the video clip\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'channel': channel,\n",
    "    'train_total_video_clip': train_total_video_clip, # This number is for KTH dataset with default setting\n",
    "    'eval_total_video_clip': eval_total_video_clip # This number is for KTH dataset with default setting\n",
    "}\n",
    "\n",
    "tfrunconfig = tf.estimator.RunConfig(\n",
    "    log_step_count_steps=1, save_summary_steps=1, model_dir=model_artifacts_location)\n",
    "\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=estimator.model_fn, params=hparams, config=tfrunconfig)\n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda: estimator.train_input_fn(data_path, hparams),\n",
    "    steps=1)\n",
    "\n",
    "classifier.evaluate(\n",
    "    input_fn=lambda: estimator.eval_input_fn(data_path, hparams),\n",
    "    steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_model = classifier.export_savedmodel(export_dir_base = '{0}/Servo/'.format(export_dir), \n",
    "                               serving_input_receiver_fn = lambda: estimator.serving_input_fn(hparams))\n",
    "\n",
    "print (exported_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add(export_dir, recursive=True)\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', bucket=bucket, \n",
    "                                       key_prefix='{0}/{1}'.format(prefix, 'model'))\n",
    "\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "action_model = TensorFlowModel(model_data= 's3://{0}/{1}/{2}'.format(bucket, prefix, 'model/model.tar.gz'),\n",
    "                                  role=role,\n",
    "                                  source_dir='source_dir',\n",
    "                                  entry_point='densenet_3d_estimator.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "action_predictor = action_model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw_video_clips = _convert_video_to_clips('test_videos/person02_running_d1_uncomp.avi')\n",
    "\n",
    "for raw_video_clip in raw_video_clips:\n",
    "    video_clip = []\n",
    "    for raw_image in raw_video_clip:\n",
    "        image = cv2.imdecode(np.frombuffer(raw_image, dtype=np.uint8), 1)\n",
    "        video_clip.append(image)\n",
    "    output_dict = action_predictor.predict( {'video_clips': [video_clip] } )\n",
    "    print(output_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.contrib import predictor\n",
    "\n",
    "predict_fn = predictor.from_saved_model(exported_model)\n",
    "\n",
    "raw_video_clips = _convert_video_to_clips('test_videos/person02_walking_d1_uncomp.avi')\n",
    "\n",
    "video_clips = []\n",
    "for idx, raw_video_clip in enumerate(raw_video_clips):\n",
    "    video_clip = []\n",
    "    for raw_image in raw_video_clip:\n",
    "        image = cv2.imdecode(np.frombuffer(raw_image, dtype=np.uint8), -1)\n",
    "        video_clip.append(image)\n",
    "    video_clips.append(video_clip)\n",
    "    if len(video_clips) == hparams['batch_size']:\n",
    "        output_dict = predict_fn( {'video_clips': video_clips } )\n",
    "        print(output_dict)\n",
    "        video_clips = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(os.path.join(os.getcwd(), 'model.tar.gz')):\n",
    "    os.remove('model.tar.gz')\n",
    "\n",
    "if os.path.exists(os.path.join(os.getcwd(), export_dir)):\n",
    "    shutil.rmtree(export_dir)\n",
    "    \n",
    "if os.path.exists(os.path.join(os.getcwd(), staging_dir)):\n",
    "    shutil.rmtree(staging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
